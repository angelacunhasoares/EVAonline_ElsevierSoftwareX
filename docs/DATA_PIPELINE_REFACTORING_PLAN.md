# ðŸ”„ Plano de AtualizaÃ§Ã£o: IntegraÃ§Ã£o Completa dos Novos Clientes

## ðŸ“Š AnÃ¡lise da SituaÃ§Ã£o Atual

### **Problema Identificado**

Os arquivos de processamento de dados (`data_download.py`, `data_preprocessing.py`, `data_fusion.py`) ainda usam os **clientes legados** (`nasapower.py`, `openmeteo.py`) e nÃ£o estÃ£o integrados com os **novos clientes cache-enabled** criados recentemente:

- âœ… `nasa_power_client.py` (global, async, cache-integrated)
- âœ… `met_norway_client.py` (Europa, async, cache-integrated)
- âœ… `nws_client.py` (USA, async, cache-integrated)

### **Incompatibilidades Detectadas**

| Aspecto | Clientes Legados | Novos Clientes | Status |
|---------|------------------|----------------|--------|
| **Interface** | `get_weather_sync()` | `async get_daily_data()` | âŒ IncompatÃ­vel |
| **Retorno** | `(DataFrame, List[str])` | `Dict[str, Any]` | âŒ Formato diferente |
| **Cache** | Redis embutido | Dependency injection | âš ï¸ HÃ­brido |
| **Cobertura** | Global/MATOPIBA | Bbox-aware | âŒ ValidaÃ§Ã£o ausente |
| **FusÃ£o** | 2 fontes fixas | FlexÃ­vel (2+) | âŒ LÃ³gica limitada |

---

## ðŸŽ¯ Objetivos

### **1. Compatibilidade**
- Adicionar mÃ©todo `get_weather_sync()` aos novos clientes
- Manter interface consistente com cÃ³digo existente

### **2. ValidaÃ§Ã£o de Cobertura**
- Verificar bbox antes de download
- Mensagens claras quando fonte indisponÃ­vel

### **3. FusÃ£o Inteligente**
- Suportar 2+ fontes simultaneamente
- Pesos baseados em prioridade + distÃ¢ncia + qualidade

### **4. PrÃ©-processamento por Fonte**
- Normalizar resoluÃ§Ã£o temporal (hourly â†’ daily)
- Harmonizar unidades (Â°F â†’ Â°C, mph â†’ m/s)

### **5. Conformidade de LicenÃ§a**
- Bloquear Open-Meteo em fusÃ£o (jÃ¡ implementado âœ…)
- Validar antes do download

---

## ðŸ“ Tarefas de ImplementaÃ§Ã£o

### **Tarefa 1: Adicionar `get_weather_sync()` aos Novos Clientes**

**Arquivos:**
- `backend/api/services/nasa_power_client.py`
- `backend/api/services/met_norway_client.py`
- `backend/api/services/nws_client.py`

**ImplementaÃ§Ã£o:**

```python
# nasa_power_client.py
def get_weather_sync(
    self,
    start: datetime,
    end: datetime,
    lat: float,
    lon: float
) -> Tuple[pd.DataFrame, List[str]]:
    """
    MÃ©todo sÃ­ncrono compatÃ­vel com data_download.py.
    
    Wrapper que chama get_daily_data() e converte resultado.
    """
    import asyncio
    
    warnings = []
    
    try:
        # Executar mÃ©todo async
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # Se jÃ¡ hÃ¡ event loop rodando (Celery), criar task
            result = asyncio.run_coroutine_threadsafe(
                self.get_daily_data(
                    lat=lat,
                    lon=lon,
                    start_date=start,
                    end_date=end
                ),
                loop
            ).result(timeout=30)
        else:
            # Se nÃ£o hÃ¡ event loop, criar novo
            result = asyncio.run(
                self.get_daily_data(
                    lat=lat,
                    lon=lon,
                    start_date=start,
                    end_date=end
                )
            )
        
        # Converter resultado para DataFrame
        if not result or 'data' not in result:
            raise ValueError("No data returned from NASA POWER API")
        
        df = pd.DataFrame(result['data'])
        df.index = pd.to_datetime(df.index)
        
        # Mapear colunas para formato esperado
        column_mapping = {
            'T2M_MAX': 'T2M_MAX',
            'T2M_MIN': 'T2M_MIN',
            'T2M': 'T2M',
            'RH2M': 'RH2M',
            'WS2M': 'WS2M',
            'ALLSKY_SFC_SW_DWN': 'ALLSKY_SFC_SW_DWN',
            'PRECTOTCORR': 'PRECTOTCORR'
        }
        
        df = df.rename(columns=column_mapping)
        
        # Warnings
        if result.get('cache_hit'):
            warnings.append("Data loaded from cache (NASA POWER)")
        
        return df, warnings
        
    except Exception as e:
        error_msg = f"NASA POWER API error: {str(e)}"
        logger.error(error_msg)
        raise ValueError(error_msg)
```

**PadrÃ£o similar para MET Norway e NWS:**
- MET: Converte dados horÃ¡rios â†’ diÃ¡rios (agregaÃ§Ã£o)
- NWS: Converte Â°F â†’ Â°C, mph â†’ m/s

---

### **Tarefa 2: Atualizar `data_download.py`**

**MudanÃ§as necessÃ¡rias:**

#### **2.1: Importar novos clientes**

```python
# REMOVER (legados)
from backend.api.services.nasapower import NasaPowerAPI
from backend.api.services.openmeteo import OpenMeteoForecastAPI

# ADICIONAR (novos)
from backend.api.services.nasa_power_client import NASAPowerClient
from backend.api.services.met_norway_client import METNorwayClient
from backend.api.services.nws_client import NWSClient
from backend.api.services.climate_source_manager import ClimateSourceManager
from backend.infrastructure.cache.climate_cache import create_climate_cache
```

#### **2.2: Validar cobertura geogrÃ¡fica**

```python
def download_weather_data(
    data_source: Union[str, List[str]],
    data_inicial: str,
    data_final: str,
    longitude: float,
    latitude: float,
) -> Tuple[pd.DataFrame, List[str]]:
    """Baixa dados meteorolÃ³gicos com validaÃ§Ã£o de cobertura."""
    
    warnings_list = []
    
    # ... validaÃ§Ãµes existentes ...
    
    # NOVO: Validar cobertura geogrÃ¡fica
    manager = ClimateSourceManager()
    available_sources = manager.get_available_sources_for_location(
        lat=latitude,
        lon=longitude,
        exclude_non_commercial=True  # Bloqueia Open-Meteo
    )
    
    # Verificar quais fontes solicitadas estÃ£o disponÃ­veis
    requested_sources = normalize_source_names(data_source)
    unavailable = []
    
    for source in requested_sources:
        if source not in available_sources:
            unavailable.append(source)
        elif not available_sources[source]['available']:
            bbox_str = available_sources[source]['bbox_str']
            unavailable.append(
                f"{source} (fora de cobertura: {bbox_str})"
            )
    
    if unavailable:
        msg = (
            f"Fontes indisponÃ­veis para ({latitude}, {longitude}): "
            f"{', '.join(unavailable)}"
        )
        warnings_list.append(msg)
        logger.warning(msg)
        
        # Remover fontes indisponÃ­veis
        requested_sources = [
            s for s in requested_sources 
            if s in available_sources and available_sources[s]['available']
        ]
    
    if not requested_sources:
        raise ValueError(
            "Nenhuma fonte de dados disponÃ­vel para esta localizaÃ§Ã£o"
        )
    
    logger.info(
        "Fontes disponÃ­veis: %s", 
        ", ".join(requested_sources)
    )
```

#### **2.3: Criar factory de clientes**

```python
def create_client(
    source: str,
    config: dict
) -> Union[NASAPowerClient, METNorwayClient, NWSClient]:
    """
    Factory para criar cliente apropriado.
    
    Args:
        source: Nome da fonte ('nasa_power', 'met_norway', 'nws_usa')
        config: ConfiguraÃ§Ã£o com cache, timeouts, etc.
    
    Returns:
        Cliente instanciado
    """
    cache = config.get('cache')
    
    if source == 'nasa_power':
        return NASAPowerClient(
            config=config.get('nasa_config', {}),
            cache=cache
        )
    elif source == 'met_norway':
        return METNorwayClient(
            config=config.get('met_config', {}),
            cache=cache
        )
    elif source == 'nws_usa':
        return NWSClient(
            config=config.get('nws_config', {}),
            cache=cache
        )
    else:
        raise ValueError(f"Unknown source: {source}")
```

#### **2.4: Loop de download atualizado**

```python
# Criar cache compartilhado
cache = create_climate_cache('shared')

dfs = []
source_metadata = []

for source in requested_sources:
    try:
        # Criar cliente
        client = create_client(source, {'cache': cache})
        
        # Download
        weather_df, fetch_warnings = client.get_weather_sync(
            start=data_inicial_formatted,
            end=data_final_adjusted,
            lat=latitude,
            lon=longitude
        )
        
        warnings_list.extend(fetch_warnings)
        
        # Validar DataFrame
        if weather_df is None or weather_df.empty:
            msg = f"{source}: No data returned"
            warnings_list.append(msg)
            continue
        
        # Armazenar metadados para fusÃ£o
        source_metadata.append({
            'name': source,
            'priority': available_sources[source]['priority'],
            'bbox': available_sources[source]['bbox'],
            'temporal': available_sources[source]['temporal']
        })
        
        dfs.append(weather_df)
        logger.info("%s: %d days downloaded", source, len(weather_df))
        
    except Exception as e:
        msg = f"{source}: Download error - {str(e)}"
        warnings_list.append(msg)
        logger.error(msg)
        continue
```

---

### **Tarefa 3: Criar ValidaÃ§Ã£o de Cobertura**

**Novo arquivo:** `backend/core/data_processing/data_validation.py`

```python
"""
MÃ³dulo de validaÃ§Ã£o de dados climÃ¡ticos.
"""

from typing import List, Dict, Tuple
from loguru import logger

from backend.api.services.climate_source_manager import ClimateSourceManager


def validate_source_coverage(
    sources: List[str],
    lat: float,
    lon: float,
    exclude_non_commercial: bool = True
) -> Tuple[List[str], List[str]]:
    """
    Valida se fontes cobrem coordenadas especificadas.
    
    Args:
        sources: Lista de IDs de fontes solicitadas
        lat: Latitude
        lon: Longitude
        exclude_non_commercial: Excluir fontes CC-BY-NC
    
    Returns:
        Tuple[List[str], List[str]]: (fontes_disponÃ­veis, warnings)
    
    Example:
        >>> available, warnings = validate_source_coverage(
        ...     ['nasa_power', 'met_norway', 'nws_usa'],
        ...     lat=48.8566,  # Paris
        ...     lon=2.3522
        ... )
        >>> # Returns: (['nasa_power', 'met_norway'], [...warnings...])
    """
    warnings = []
    manager = ClimateSourceManager()
    
    # Obter fontes disponÃ­veis
    available_dict = manager.get_available_sources_for_location(
        lat=lat,
        lon=lon,
        exclude_non_commercial=exclude_non_commercial
    )
    
    # Validar cada fonte solicitada
    validated_sources = []
    
    for source in sources:
        if source not in available_dict:
            msg = (
                f"âš ï¸ {source}: Fonte desconhecida ou bloqueada "
                f"(licenÃ§a nÃ£o-comercial)"
            )
            warnings.append(msg)
            logger.warning(msg)
            continue
        
        source_info = available_dict[source]
        
        if not source_info['available']:
            bbox_str = source_info.get('bbox_str', 'Unknown coverage')
            msg = (
                f"âš ï¸ {source}: Coordenadas ({lat}, {lon}) fora de "
                f"cobertura. {bbox_str}"
            )
            warnings.append(msg)
            logger.warning(msg)
            continue
        
        # Fonte vÃ¡lida
        validated_sources.append(source)
        logger.info(
            "âœ… %s: Available for (%.4f, %.4f)",
            source, lat, lon
        )
    
    return validated_sources, warnings
```

---

### **Tarefa 4: Atualizar LÃ³gica de FusÃ£o**

**MudanÃ§as em `data_download.py`:**

```python
# FusÃ£o dinÃ¢mica (2+ fontes)
if len(requested_sources) >= 2:
    if len(dfs) < 2:
        msg = "FusÃ£o requer pelo menos 2 fontes vÃ¡lidas"
        logger.error(msg)
        raise ValueError(msg)
    
    try:
        # Converte DataFrames para dicionÃ¡rios
        df_dicts = []
        for df in dfs:
            df_dict = {}
            for idx, row in df.iterrows():
                key = idx.strftime("%Y-%m-%d %H:%M:%S")
                df_dict[key] = row.to_dict()
            df_dicts.append(df_dict)
        
        # Executa fusÃ£o com validaÃ§Ã£o de licenÃ§a
        task = data_fusion.delay(
            df_dicts,
            source_names=requested_sources,  # ValidaÃ§Ã£o
            source_metadata=source_metadata  # Para pesos inteligentes
        )
        weather_data_dict, fusion_warnings = task.get(timeout=30)
        warnings_list.extend(fusion_warnings)
        
        # Converte resultado para DataFrame
        weather_data = pd.DataFrame.from_dict(
            weather_data_dict,
            orient='index'
        )
        weather_data.index = pd.to_datetime(weather_data.index)
        
        logger.info(
            "âœ… Data fusion completed: %s",
            " + ".join(requested_sources)
        )
        
    except Exception as e:
        msg = f"Erro na fusÃ£o de dados: {str(e)}"
        logger.error(msg)
        raise ValueError(msg)

else:
    # Fonte Ãºnica
    if not dfs:
        raise ValueError("Nenhuma fonte forneceu dados vÃ¡lidos")
    weather_data = dfs[0]
    logger.info("âœ… Single source data: %s", requested_sources[0])
```

---

### **Tarefa 5: PrÃ©-processamento por Fonte**

**Novo arquivo:** `backend/core/data_processing/source_preprocessors.py`

```python
"""
PrÃ©-processadores especÃ­ficos por fonte de dados.

Cada fonte tem caracterÃ­sticas prÃ³prias:
- NASA POWER: DiÃ¡rio, global, MJ/mÂ²/dia
- MET Norway: HorÃ¡rio, Europa, W/mÂ²
- NWS: HorÃ¡rio, USA, Â°F/mph
"""

import pandas as pd
import numpy as np
from typing import Tuple, List
from loguru import logger


def preprocess_nasa_power(
    df: pd.DataFrame
) -> Tuple[pd.DataFrame, List[str]]:
    """
    PrÃ©-processa dados NASA POWER.
    
    NASA POWER jÃ¡ retorna:
    - ResoluÃ§Ã£o diÃ¡ria
    - Temperatura em Â°C
    - RadiaÃ§Ã£o em MJ/mÂ²/dia
    - Velocidade do vento em m/s
    
    Args:
        df: DataFrame com dados NASA POWER
    
    Returns:
        Tuple[pd.DataFrame, List[str]]: DataFrame processado e warnings
    """
    warnings = []
    
    # NASA POWER jÃ¡ estÃ¡ no formato correto
    # Apenas validar colunas esperadas
    expected_cols = [
        'T2M_MAX', 'T2M_MIN', 'T2M', 'RH2M', 
        'WS2M', 'ALLSKY_SFC_SW_DWN', 'PRECTOTCORR'
    ]
    
    missing = set(expected_cols) - set(df.columns)
    if missing:
        warnings.append(
            f"NASA POWER: Missing columns {missing}"
        )
    
    return df, warnings


def preprocess_met_norway(
    df: pd.DataFrame
) -> Tuple[pd.DataFrame, List[str]]:
    """
    PrÃ©-processa dados MET Norway.
    
    MET Norway retorna dados horÃ¡rios:
    - Temperatura em Â°C (OK)
    - RadiaÃ§Ã£o em W/mÂ² â†’ precisa converter para MJ/mÂ²/dia
    - Vento em m/s (OK)
    
    AgregaÃ§Ã£o hourly â†’ daily:
    - T2M_MAX: max diÃ¡rio
    - T2M_MIN: min diÃ¡rio
    - T2M: mÃ©dia diÃ¡ria
    - ALLSKY_SFC_SW_DWN: soma horÃ¡ria * 3600 / 1000000
    
    Args:
        df: DataFrame com dados horÃ¡rios MET Norway
    
    Returns:
        Tuple[pd.DataFrame, List[str]]: DataFrame diÃ¡rio e warnings
    """
    warnings = []
    
    # Converter Ã­ndice para date (agregaÃ§Ã£o diÃ¡ria)
    df['date'] = df.index.date
    
    # AgregaÃ§Ãµes
    daily_df = pd.DataFrame()
    
    # Temperatura
    if 'air_temperature' in df.columns:
        daily_df['T2M_MAX'] = df.groupby('date')['air_temperature'].max()
        daily_df['T2M_MIN'] = df.groupby('date')['air_temperature'].min()
        daily_df['T2M'] = df.groupby('date')['air_temperature'].mean()
    
    # Umidade
    if 'relative_humidity' in df.columns:
        daily_df['RH2M'] = df.groupby('date')['relative_humidity'].mean()
    
    # Vento
    if 'wind_speed' in df.columns:
        daily_df['WS2M'] = df.groupby('date')['wind_speed'].mean()
    
    # RadiaÃ§Ã£o solar: W/mÂ² â†’ MJ/mÂ²/dia
    # 1 W/mÂ² * 1 hora = 3600 J/mÂ² = 0.0036 MJ/mÂ²
    if 'solar_radiation' in df.columns:
        # Soma de todas as horas * 3600 / 1e6
        daily_df['ALLSKY_SFC_SW_DWN'] = (
            df.groupby('date')['solar_radiation'].sum() * 3600 / 1e6
        )
    
    # PrecipitaÃ§Ã£o
    if 'precipitation_amount' in df.columns:
        daily_df['PRECTOTCORR'] = (
            df.groupby('date')['precipitation_amount'].sum()
        )
    
    # Converter Ã­ndice para datetime
    daily_df.index = pd.to_datetime(daily_df.index)
    
    warnings.append(
        f"MET Norway: Aggregated {len(df)} hourly records "
        f"to {len(daily_df)} daily records"
    )
    
    return daily_df, warnings


def preprocess_nws(
    df: pd.DataFrame
) -> Tuple[pd.DataFrame, List[str]]:
    """
    PrÃ©-processa dados NWS/NOAA.
    
    NWS retorna dados horÃ¡rios em unidades imperiais:
    - Temperatura em Â°F â†’ converter para Â°C
    - Vento em mph â†’ converter para m/s
    
    ConversÃµes:
    - Â°C = (Â°F - 32) * 5/9
    - m/s = mph * 0.44704
    
    Args:
        df: DataFrame com dados horÃ¡rios NWS
    
    Returns:
        Tuple[pd.DataFrame, List[str]]: DataFrame diÃ¡rio e warnings
    """
    warnings = []
    
    # Converter temperaturas Â°F â†’ Â°C
    if 'temperature' in df.columns:
        df['temperature'] = (df['temperature'] - 32) * 5 / 9
    
    if 'dewpoint' in df.columns:
        df['dewpoint'] = (df['dewpoint'] - 32) * 5 / 9
    
    # Converter vento mph â†’ m/s
    if 'wind_speed' in df.columns:
        # Remover "mph" da string e converter
        df['wind_speed'] = df['wind_speed'].str.extract(
            r'(\d+)'
        ).astype(float) * 0.44704
    
    # AgregaÃ§Ã£o diÃ¡ria
    df['date'] = df.index.date
    
    daily_df = pd.DataFrame()
    
    # Temperatura
    if 'temperature' in df.columns:
        daily_df['T2M_MAX'] = df.groupby('date')['temperature'].max()
        daily_df['T2M_MIN'] = df.groupby('date')['temperature'].min()
        daily_df['T2M'] = df.groupby('date')['temperature'].mean()
    
    # Umidade (calcular de dewpoint)
    if 'dewpoint' in df.columns and 'temperature' in df.columns:
        # Magnus formula: RH = 100 * exp((17.625*Td)/(243.04+Td)) / 
        #                       exp((17.625*T)/(243.04+T))
        df['RH'] = 100 * np.exp(
            (17.625 * df['dewpoint']) / (243.04 + df['dewpoint'])
        ) / np.exp(
            (17.625 * df['temperature']) / (243.04 + df['temperature'])
        )
        daily_df['RH2M'] = df.groupby('date')['RH'].mean()
    
    # Vento
    if 'wind_speed' in df.columns:
        daily_df['WS2M'] = df.groupby('date')['wind_speed'].mean()
    
    # PrecipitaÃ§Ã£o
    if 'quantitative_precipitation' in df.columns:
        daily_df['PRECTOTCORR'] = (
            df.groupby('date')['quantitative_precipitation'].sum()
        )
    
    # RadiaÃ§Ã£o solar (estimativa de cloud cover)
    # NWS nÃ£o fornece radiaÃ§Ã£o diretamente
    # Usar cloud_cover para estimar (aproximaÃ§Ã£o)
    if 'sky_cover' in df.columns:
        # Placeholder: requer modelo mais sofisticado
        warnings.append(
            "NWS: Solar radiation estimated from cloud cover "
            "(may have higher uncertainty)"
        )
    
    daily_df.index = pd.to_datetime(daily_df.index)
    
    warnings.append(
        f"NWS: Converted imperial to metric units, "
        f"aggregated {len(df)} hourly to {len(daily_df)} daily records"
    )
    
    return daily_df, warnings


def preprocess_by_source(
    df: pd.DataFrame,
    source: str
) -> Tuple[pd.DataFrame, List[str]]:
    """
    Dispatch para prÃ©-processador apropriado.
    
    Args:
        df: DataFrame com dados brutos
        source: ID da fonte ('nasa_power', 'met_norway', 'nws_usa')
    
    Returns:
        Tuple[pd.DataFrame, List[str]]: DataFrame processado e warnings
    """
    if source == 'nasa_power':
        return preprocess_nasa_power(df)
    elif source == 'met_norway':
        return preprocess_met_norway(df)
    elif source == 'nws_usa':
        return preprocess_nws(df)
    else:
        # Fonte desconhecida, retornar sem alteraÃ§Ãµes
        return df, [f"Warning: Unknown source {source}, no preprocessing applied"]
```

**IntegraÃ§Ã£o em `data_download.py`:**

```python
from backend.core.data_processing.source_preprocessors import preprocess_by_source

# ApÃ³s download, antes de adicionar ao dfs
for source in requested_sources:
    # ... download ...
    
    # PRÃ‰-PROCESSAMENTO POR FONTE
    weather_df, preproc_warnings = preprocess_by_source(
        weather_df,
        source
    )
    warnings_list.extend(preproc_warnings)
    
    dfs.append(weather_df)
```

---

### **Tarefa 6: Pesos Inteligentes de FusÃ£o**

**Atualizar `data_fusion.py`:**

```python
@shared_task(bind=True, name='src.data_fusion.data_fusion')
def data_fusion(
    self,
    dfs: List[dict],
    ensemble_size: int = 50,
    inflation_factor: float = 1.02,
    source_names: Optional[List[str]] = None,
    source_metadata: Optional[List[Dict]] = None  # NOVO
) -> Tuple[Dict[str, Any], List[str]]:
    """
    FusÃ£o com pesos inteligentes baseados em:
    - Prioridade da fonte (SOURCES_CONFIG)
    - DistÃ¢ncia do centro de cobertura
    - Qualidade histÃ³rica (se disponÃ­vel)
    """
    
    # ... validaÃ§Ã£o de licenÃ§a existente ...
    
    # NOVO: Calcular pesos inteligentes
    if source_metadata:
        weights = calculate_fusion_weights(
            source_metadata,
            location=(lat, lon)  # Passar coords se disponÃ­vel
        )
        logger.info("Fusion weights: %s", weights)
    else:
        # Fallback: pesos iguais
        weights = [1.0 / len(dfs)] * len(dfs)
    
    # Aplicar pesos no ensemble
    forecast = np.average(
        [df.to_numpy() for df in dataframes],
        axis=0,
        weights=weights
    )
    
    # ... resto do algoritmo Kalman ...
```

**Nova funÃ§Ã£o de cÃ¡lculo de pesos:**

```python
def calculate_fusion_weights(
    source_metadata: List[Dict],
    location: Optional[Tuple[float, float]] = None
) -> List[float]:
    """
    Calcula pesos para fusÃ£o baseado em mÃºltiplos critÃ©rios.
    
    CritÃ©rios:
    1. Prioridade (40%): Menor prioridade = maior peso
    2. DistÃ¢ncia (30%): Mais prÃ³ximo do centro de cobertura = maior peso
    3. ResoluÃ§Ã£o temporal (20%): Hourly > Daily
    4. Qualidade histÃ³rica (10%): Baseado em validaÃ§Ãµes passadas
    
    Args:
        source_metadata: Lista de metadados das fontes
        location: Tupla (lat, lon) opcional para cÃ¡lculo de distÃ¢ncia
    
    Returns:
        List[float]: Pesos normalizados (soma = 1.0)
    """
    import math
    
    weights = []
    
    for meta in source_metadata:
        # 1. Peso de prioridade (inverso)
        priority = meta.get('priority', 5)
        priority_weight = 1.0 / priority
        
        # 2. Peso de distÃ¢ncia
        if location and meta.get('bbox'):
            bbox = meta['bbox']
            # Calcular centro do bbox
            center_lat = (bbox[1] + bbox[3]) / 2
            center_lon = (bbox[0] + bbox[2]) / 2
            
            # DistÃ¢ncia haversine simplificada
            dlat = math.radians(location[0] - center_lat)
            dlon = math.radians(location[1] - center_lon)
            a = (math.sin(dlat/2)**2 + 
                 math.cos(math.radians(center_lat)) *
                 math.cos(math.radians(location[0])) *
                 math.sin(dlon/2)**2)
            distance = 2 * math.asin(math.sqrt(a)) * 6371  # km
            
            # Inverso da distÃ¢ncia (mais prÃ³ximo = maior peso)
            distance_weight = 1.0 / (1.0 + distance / 1000)
        else:
            distance_weight = 1.0  # Sem informaÃ§Ã£o, peso neutro
        
        # 3. Peso de resoluÃ§Ã£o temporal
        temporal = meta.get('temporal', 'daily')
        temporal_weight = 1.2 if temporal == 'hourly' else 1.0
        
        # 4. Combinar pesos (mÃ©dia ponderada)
        combined_weight = (
            0.4 * priority_weight +
            0.3 * distance_weight +
            0.2 * temporal_weight +
            0.1 * 1.0  # Qualidade (placeholder)
        )
        
        weights.append(combined_weight)
    
    # Normalizar (soma = 1.0)
    total = sum(weights)
    normalized_weights = [w / total for w in weights]
    
    logger.info(
        "Fusion weights: %s",
        {meta['name']: f"{w:.3f}" 
         for meta, w in zip(source_metadata, normalized_weights)}
    )
    
    return normalized_weights
```

---

### **Tarefa 7: Testes de IntegraÃ§Ã£o**

**Novo arquivo:** `backend/tests/integration/test_data_pipeline.py`

```python
"""
Testes de integraÃ§Ã£o do pipeline completo.
"""

import pytest
import pandas as pd
from datetime import datetime, timedelta

from backend.core.data_processing.data_download import download_weather_data
from backend.core.data_processing.data_preprocessing import preprocessing
from backend.core.eto_calculation.eto_calculation import calculate_eto


class TestDataPipeline:
    """Testes end-to-end do pipeline de dados."""
    
    @pytest.mark.integration
    def test_pipeline_paris_nasa_met(self):
        """
        Testa pipeline completo para Paris com fusÃ£o NASA + MET.
        """
        # Paris, FranÃ§a
        lat, lon = 48.8566, 2.3522
        start = (datetime.now() - timedelta(days=10)).strftime('%Y-%m-%d')
        end = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
        
        # Download com fusÃ£o
        df, warnings = download_weather_data(
            data_source=['nasa_power', 'met_norway'],
            data_inicial=start,
            data_final=end,
            latitude=lat,
            longitude=lon
        )
        
        # ValidaÃ§Ãµes
        assert df is not None
        assert not df.empty
        assert 'T2M_MAX' in df.columns
        assert 'ALLSKY_SFC_SW_DWN' in df.columns
        
        # Preprocessing
        df_clean, prep_warnings = preprocessing(df, latitude=lat)
        
        assert df_clean is not None
        assert not df_clean.isna().all().any()  # Sem colunas totalmente NaN
        
        # CÃ¡lculo de ETo
        eto_results, eto_warnings = calculate_eto(df_clean, latitude=lat)
        
        assert eto_results is not None
        assert 'ETo' in eto_results.columns
        assert (eto_results['ETo'] > 0).all()  # ETo positivo
        assert (eto_results['ETo'] < 20).all()  # ETo razoÃ¡vel (<20 mm/dia)
    
    @pytest.mark.integration
    def test_pipeline_new_york_nasa_nws(self):
        """
        Testa pipeline para Nova York com fusÃ£o NASA + NWS.
        """
        # Nova York, USA
        lat, lon = 40.7128, -74.0060
        start = (datetime.now() - timedelta(days=10)).strftime('%Y-%m-%d')
        end = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
        
        df, warnings = download_weather_data(
            data_source=['nasa_power', 'nws_usa'],
            data_inicial=start,
            data_final=end,
            latitude=lat,
            longitude=lon
        )
        
        assert df is not None
        assert not df.empty
    
    @pytest.mark.integration
    def test_coverage_validation_blocks_wrong_source(self):
        """
        Testa que validaÃ§Ã£o bloqueia fontes fora de cobertura.
        """
        # Balsas, MA (Brasil) - fora de Europa e USA
        lat, lon = -7.5312, -46.0390
        
        # Tentar usar MET Norway (Europa apenas)
        with pytest.raises(ValueError, match="Nenhuma fonte.*disponÃ­vel"):
            download_weather_data(
                data_source=['met_norway'],  # Europa apenas
                data_inicial='2024-01-01',
                data_final='2024-01-07',
                latitude=lat,
                longitude=lon
            )
    
    @pytest.mark.integration
    def test_license_validation_blocks_openmeteo_fusion(self):
        """
        Testa que Open-Meteo Ã© bloqueado em fusÃ£o.
        """
        lat, lon = -7.5312, -46.0390  # Balsas, MA
        
        # Tentar fusÃ£o com Open-Meteo
        with pytest.raises(ValueError, match="LICENSE VIOLATION"):
            download_weather_data(
                data_source=['nasa_power', 'openmeteo_forecast'],
                data_inicial='2024-01-01',
                data_final='2024-01-07',
                latitude=lat,
                longitude=lon
            )
```

---

## ðŸ“Š Cronograma de ImplementaÃ§Ã£o

| Tarefa | EsforÃ§o | Prioridade | DependÃªncias |
|--------|---------|------------|--------------|
| 1. `get_weather_sync()` | 4h | ðŸ”´ Alta | Nenhuma |
| 2. Atualizar `data_download.py` | 6h | ðŸ”´ Alta | Tarefa 1 |
| 3. ValidaÃ§Ã£o de cobertura | 3h | ðŸŸ¡ MÃ©dia | Tarefa 2 |
| 4. FusÃ£o mÃºltiplas fontes | 3h | ðŸŸ¡ MÃ©dia | Tarefas 2, 3 |
| 5. PrÃ©-processamento por fonte | 5h | ðŸ”´ Alta | Tarefa 2 |
| 6. Pesos inteligentes | 4h | ðŸŸ¢ Baixa | Tarefa 4 |
| 7. Testes de integraÃ§Ã£o | 4h | ðŸ”´ Alta | Todas |

**Total:** ~29 horas de desenvolvimento

---

## âœ… Checklist de ValidaÃ§Ã£o

Antes de considerar completo, validar:

- [ ] `get_weather_sync()` implementado nos 3 novos clientes
- [ ] `data_download.py` usa novos clientes (nÃ£o legados)
- [ ] ValidaÃ§Ã£o de cobertura geogrÃ¡fica funcional
- [ ] FusÃ£o suporta 2+ fontes dinamicamente
- [ ] PrÃ©-processamento normaliza unidades e resoluÃ§Ã£o
- [ ] Pesos de fusÃ£o consideram prioridade + distÃ¢ncia
- [ ] ValidaÃ§Ã£o de licenÃ§a bloqueia Open-Meteo
- [ ] Testes de integraÃ§Ã£o passando (Paris, NYC, Balsas)
- [ ] Logs informativos em cada etapa
- [ ] DocumentaÃ§Ã£o atualizada

---

**PrÃ³ximos Passos:**
1. Implementar `get_weather_sync()` nos 3 clientes âœ…
2. Refatorar `data_download.py` completamente
3. Criar `source_preprocessors.py` e `data_validation.py`
4. Adicionar pesos inteligentes em `data_fusion.py`
5. Escrever testes de integraÃ§Ã£o
6. Validar com dataset Xavier (17 cidades MATOPIBA)

**Objetivo Final:** Pipeline robusto que suporta mÃºltiplas fontes, valida cobertura automaticamente, aplica prÃ©-processamento apropriado, e funde dados com pesos inteligentes - tudo com proteÃ§Ãµes de conformidade de licenÃ§a.
